Number of batches:  53220
[train] epoch=0 step=50 loss=2.7146 lr=5.00e-05
[train] epoch=0 step=100 loss=2.5745 lr=9.91e-05
[train] epoch=0 step=150 loss=2.4248 lr=1.48e-04
[train] epoch=0 step=200 loss=2.2594 lr=1.97e-04
[train] epoch=0 step=250 loss=2.3076 lr=2.46e-04
[train] epoch=0 step=300 loss=2.3173 lr=2.95e-04
[train] epoch=0 step=350 loss=2.3147 lr=3.44e-04
[train] epoch=0 step=400 loss=2.2198 lr=3.93e-04
[train] epoch=0 step=450 loss=2.2361 lr=4.42e-04
[train] epoch=0 step=500 loss=2.2415 lr=4.91e-04
[train] epoch=0 step=550 loss=2.0796 lr=5.40e-04
[train] epoch=0 step=600 loss=2.2239 lr=5.89e-04
[train] epoch=0 step=650 loss=2.1403 lr=6.39e-04
[train] epoch=0 step=700 loss=2.1739 lr=6.88e-04
[train] epoch=0 step=750 loss=2.3342 lr=7.37e-04
[train] epoch=0 step=800 loss=2.1907 lr=7.86e-04
[train] epoch=0 step=850 loss=2.1738 lr=8.35e-04
[train] epoch=0 step=900 loss=2.2999 lr=8.84e-04
[train] epoch=0 step=950 loss=2.2083 lr=9.33e-04
[train] epoch=0 step=1000 loss=2.2206 lr=9.81e-04
[train] epoch=0 step=1050 loss=2.2279 lr=9.69e-04
[train] epoch=0 step=1100 loss=2.2728 lr=9.56e-04
[train] epoch=0 step=1150 loss=2.1418 lr=9.44e-04
[train] epoch=0 step=1200 loss=2.2324 lr=9.32e-04
[train] epoch=0 step=1250 loss=2.2248 lr=9.20e-04
[train] epoch=0 step=1300 loss=2.1690 lr=9.07e-04
[train] epoch=0 step=1350 loss=2.3034 lr=8.95e-04
[train] epoch=0 step=1400 loss=2.1706 lr=8.83e-04
[train] epoch=0 step=1450 loss=1.9968 lr=8.70e-04
[train] epoch=0 step=1500 loss=2.1401 lr=8.58e-04
[train] epoch=0 step=1550 loss=2.2525 lr=8.46e-04
[train] epoch=0 step=1600 loss=2.1400 lr=8.34e-04
[train] epoch=0 step=1650 loss=2.0254 lr=8.21e-04
[train] epoch=0 step=1700 loss=2.1231 lr=8.09e-04
[train] epoch=0 step=1750 loss=2.1408 lr=7.97e-04
[train] epoch=0 step=1800 loss=2.1952 lr=7.85e-04
[train] epoch=0 step=1850 loss=2.1908 lr=7.72e-04
[train] epoch=0 step=1900 loss=2.2521 lr=7.60e-04
[train] epoch=0 step=1950 loss=2.1804 lr=7.48e-04
[train] epoch=0 step=2000 loss=2.2338 lr=7.36e-04
[train] epoch=0 step=2050 loss=2.1691 lr=7.23e-04
[train] epoch=0 step=2100 loss=2.1590 lr=7.11e-04
[train] epoch=0 step=2150 loss=2.0892 lr=6.99e-04
[train] epoch=0 step=2200 loss=2.2581 lr=6.87e-04
[train] epoch=0 step=2250 loss=2.0972 lr=6.74e-04
[train] epoch=0 step=2300 loss=2.2476 lr=6.62e-04
[train] epoch=0 step=2350 loss=2.2498 lr=6.50e-04
[train] epoch=0 step=2400 loss=2.2096 lr=6.38e-04
[train] epoch=0 step=2450 loss=2.1435 lr=6.25e-04
[train] epoch=0 step=2500 loss=2.2310 lr=6.13e-04
[train] epoch=0 step=2550 loss=2.2722 lr=6.01e-04
[train] epoch=0 step=2600 loss=2.2195 lr=5.88e-04
[train] epoch=0 step=2650 loss=2.1614 lr=5.76e-04
[train] epoch=0 step=2700 loss=2.1784 lr=5.64e-04
[train] epoch=0 step=2750 loss=2.2248 lr=5.52e-04
[train] epoch=0 step=2800 loss=2.2347 lr=5.39e-04
[train] epoch=0 step=2850 loss=2.1698 lr=5.27e-04
[train] epoch=0 step=2900 loss=2.1159 lr=5.15e-04
[train] epoch=0 step=2950 loss=2.1027 lr=5.03e-04
[train] epoch=0 step=3000 loss=2.1623 lr=4.90e-04
[train] epoch=0 step=3050 loss=2.1558 lr=4.78e-04
[train] epoch=0 step=3100 loss=2.1165 lr=4.66e-04
[train] epoch=0 step=3150 loss=2.1531 lr=4.54e-04
[train] epoch=0 step=3200 loss=2.1268 lr=4.41e-04
[train] epoch=0 step=3250 loss=2.1558 lr=4.29e-04
[train] epoch=0 step=3300 loss=2.1521 lr=4.17e-04
[train] epoch=0 step=3350 loss=2.0859 lr=4.05e-04
[train] epoch=0 step=3400 loss=2.1699 lr=3.92e-04
[train] epoch=0 step=3450 loss=2.0314 lr=3.80e-04
[train] epoch=0 step=3500 loss=2.1860 lr=3.68e-04
[train] epoch=0 step=3550 loss=2.0389 lr=3.56e-04
[train] epoch=0 step=3600 loss=2.1861 lr=3.43e-04
[train] epoch=0 step=3650 loss=2.0934 lr=3.31e-04
[train] epoch=0 step=3700 loss=2.1254 lr=3.19e-04
[train] epoch=0 step=3750 loss=2.1638 lr=3.07e-04
[train] epoch=0 step=3800 loss=2.1315 lr=2.94e-04
[train] epoch=0 step=3850 loss=2.2098 lr=2.82e-04
[train] epoch=0 step=3900 loss=1.9833 lr=2.70e-04
[train] epoch=0 step=3950 loss=2.1731 lr=2.57e-04
[train] epoch=0 step=4000 loss=2.1879 lr=2.45e-04
[train] epoch=0 step=4050 loss=2.0358 lr=2.33e-04
[train] epoch=0 step=4100 loss=2.1414 lr=2.21e-04
[train] epoch=0 step=4150 loss=2.2283 lr=2.08e-04
[train] epoch=0 step=4200 loss=2.1295 lr=1.96e-04
[train] epoch=0 step=4250 loss=2.1319 lr=1.84e-04
[train] epoch=0 step=4300 loss=2.1018 lr=1.72e-04
[train] epoch=0 step=4350 loss=2.2315 lr=1.59e-04
[train] epoch=0 step=4400 loss=2.1740 lr=1.47e-04
[train] epoch=0 step=4450 loss=1.9807 lr=1.35e-04
[train] epoch=0 step=4500 loss=2.1629 lr=1.23e-04
[train] epoch=0 step=4550 loss=2.2233 lr=1.10e-04
[train] epoch=0 step=4600 loss=2.2229 lr=9.81e-05
[train] epoch=0 step=4650 loss=2.3309 lr=8.58e-05
[train] epoch=0 step=4700 loss=2.1653 lr=7.36e-05
[train] epoch=0 step=4750 loss=2.1882 lr=6.13e-05
[train] epoch=0 step=4800 loss=2.2361 lr=4.90e-05
[train] epoch=0 step=4850 loss=2.1647 lr=3.68e-05
[train] epoch=0 step=4900 loss=2.2099 lr=2.45e-05
[train] epoch=0 step=4950 loss=2.1177 lr=1.23e-05
[train] epoch=0 step=5000 loss=2.2355 lr=0.00e+00
[val] epoch=0 val_loss=2.1509
[INFO] Saved checkpoint to checkpoints/encoder_pretrain_2026-01-12_16-52-18/best_encoder.pt (step=5000, val_loss=2.1509)
[INFO] Saved loss plot to checkpoints/encoder_pretrain_2026-01-12_16-52-18/training_losses.png

[INFO] Computing final evaluation metrics...

======================================================================
ENCODER CAPACITY EVALUATION METRICS
======================================================================

ðŸ“Š Reconstruction Performance:
  â€¢ Loss:              2.1698
  â€¢ Perplexity:        8.76
  â€¢ Token Accuracy:    29.77%
  â€¢ Sequence Accuracy: 0.00%

ðŸ”¬ Latent Space Statistics:
  â€¢ Mean (mu):         0.0003
  â€¢ Std (mu):          1.0481
  â€¢ Mean Variance:     0.7339
  â€¢ Pairwise Distance: 34.1374

ðŸ“ˆ Samples Evaluated: 1008
======================================================================

[INFO] Saved evaluation metrics to checkpoints/encoder_pretrain_2026-01-12_16-52-18/evaluation_metrics.json

[INFO] Collecting latent space samples for visualization...
[INFO] Saved latent distribution plot to checkpoints/encoder_pretrain_2026-01-12_16-52-18/latent_distribution.png

[INFO] Pretraining complete! Checkpoints and plots saved to checkpoints/encoder_pretrain_2026-01-12_16-52-18
