nohup: ignoring input
[train] epoch=0 step=50 loss=3.5239 lr=1.28e-06
[train] epoch=0 step=100 loss=2.9078 lr=2.53e-06
[train] epoch=0 step=150 loss=2.7098 lr=3.77e-06
[train] epoch=0 step=200 loss=2.6153 lr=5.03e-06
[train] epoch=0 step=250 loss=2.5136 lr=6.28e-06
[train] epoch=0 step=300 loss=2.4944 lr=7.52e-06
[train] epoch=0 step=350 loss=2.5326 lr=8.77e-06
[train] epoch=0 step=400 loss=2.4705 lr=1.00e-05
[train] epoch=0 step=450 loss=2.3794 lr=1.13e-05
[train] epoch=0 step=500 loss=2.4520 lr=1.25e-05
[train] epoch=0 step=550 loss=2.4213 lr=1.38e-05
[train] epoch=0 step=600 loss=2.4284 lr=1.50e-05
[train] epoch=0 step=650 loss=2.3915 lr=1.63e-05
[train] epoch=0 step=700 loss=2.3511 lr=1.75e-05
[train] epoch=0 step=750 loss=2.3614 lr=1.88e-05
[train] epoch=0 step=800 loss=2.3949 lr=2.00e-05
[train] epoch=0 step=850 loss=2.2968 lr=2.13e-05
[train] epoch=0 step=900 loss=2.3719 lr=2.25e-05
[train] epoch=0 step=950 loss=2.3514 lr=2.38e-05
[train] epoch=0 step=1000 loss=2.2537 lr=2.50e-05
[train] epoch=0 step=1050 loss=2.2718 lr=2.63e-05
[train] epoch=0 step=1100 loss=2.2345 lr=2.75e-05
[train] epoch=0 step=1150 loss=2.1948 lr=2.88e-05
[train] epoch=0 step=1200 loss=2.1898 lr=3.00e-05
[train] epoch=0 step=1250 loss=2.2176 lr=3.13e-05
[train] epoch=0 step=1300 loss=2.1868 lr=3.25e-05
[train] epoch=0 step=1350 loss=2.2112 lr=3.38e-05
[train] epoch=0 step=1400 loss=2.1439 lr=3.50e-05
[train] epoch=0 step=1450 loss=2.2364 lr=3.63e-05
[train] epoch=0 step=1500 loss=2.2228 lr=3.75e-05
[train] epoch=0 step=1550 loss=2.2068 lr=3.88e-05
[train] epoch=0 step=1600 loss=2.1816 lr=4.00e-05
[train] epoch=0 step=1650 loss=2.1575 lr=4.13e-05
[train] epoch=0 step=1700 loss=2.1868 lr=4.25e-05
[train] epoch=0 step=1750 loss=2.2117 lr=4.38e-05
[train] epoch=0 step=1800 loss=2.2190 lr=4.50e-05
[train] epoch=0 step=1850 loss=2.1387 lr=4.63e-05
[train] epoch=0 step=1900 loss=2.1526 lr=4.75e-05
[train] epoch=0 step=1950 loss=2.2210 lr=4.88e-05
[train] epoch=0 step=2000 loss=2.1596 lr=5.00e-05
[train] epoch=0 step=2050 loss=2.2254 lr=5.13e-05
[train] epoch=0 step=2100 loss=2.1247 lr=5.25e-05
[train] epoch=0 step=2150 loss=2.1527 lr=5.38e-05
[train] epoch=0 step=2200 loss=2.1074 lr=5.50e-05
[train] epoch=0 step=2250 loss=2.1484 lr=5.63e-05
[train] epoch=0 step=2300 loss=2.1729 lr=5.75e-05
[train] epoch=0 step=2350 loss=2.1201 lr=5.88e-05
[train] epoch=0 step=2400 loss=2.2132 lr=6.00e-05
[train] epoch=0 step=2450 loss=2.2143 lr=6.13e-05
[train] epoch=0 step=2500 loss=2.1218 lr=6.25e-05
[train] epoch=0 step=2550 loss=2.1581 lr=6.38e-05
[train] epoch=0 step=2600 loss=2.1822 lr=6.50e-05
[train] epoch=0 step=2650 loss=2.1966 lr=6.63e-05
[train] epoch=0 step=2700 loss=2.1865 lr=6.75e-05
[train] epoch=0 step=2750 loss=2.0491 lr=6.88e-05
[train] epoch=0 step=2800 loss=2.1910 lr=7.00e-05
[train] epoch=0 step=2850 loss=2.1477 lr=7.13e-05
[train] epoch=0 step=2900 loss=2.1772 lr=7.25e-05
[train] epoch=0 step=2950 loss=2.2478 lr=7.38e-05
[train] epoch=0 step=3000 loss=2.1082 lr=7.50e-05
[train] epoch=0 step=3050 loss=2.2145 lr=7.63e-05
[train] epoch=0 step=3100 loss=2.0933 lr=7.75e-05
[train] epoch=0 step=3150 loss=2.1844 lr=7.88e-05
[train] epoch=0 step=3200 loss=2.0539 lr=8.00e-05
[train] epoch=0 step=3250 loss=2.1508 lr=8.13e-05
[train] epoch=0 step=3300 loss=2.1545 lr=8.25e-05
[train] epoch=0 step=3350 loss=2.1153 lr=8.38e-05
[train] epoch=0 step=3400 loss=2.1514 lr=8.50e-05
[train] epoch=0 step=3450 loss=2.2601 lr=8.63e-05
[train] epoch=0 step=3500 loss=2.1847 lr=8.75e-05
[train] epoch=0 step=3550 loss=2.1693 lr=8.88e-05
[train] epoch=0 step=3600 loss=2.1018 lr=9.00e-05
[train] epoch=0 step=3650 loss=2.0841 lr=9.13e-05
[train] epoch=0 step=3700 loss=2.2060 lr=9.25e-05
[train] epoch=0 step=3750 loss=2.1210 lr=9.38e-05
[train] epoch=0 step=3800 loss=2.1314 lr=9.50e-05
[train] epoch=0 step=3850 loss=2.1132 lr=9.63e-05
[train] epoch=0 step=3900 loss=2.1840 lr=9.75e-05
[train] epoch=0 step=3950 loss=2.1051 lr=9.88e-05
[train] epoch=0 step=4000 loss=2.2063 lr=1.00e-04
[train] epoch=0 step=4050 loss=2.1734 lr=9.50e-05
[train] epoch=0 step=4100 loss=2.1609 lr=9.00e-05
[train] epoch=0 step=4150 loss=2.1571 lr=8.50e-05
[train] epoch=0 step=4200 loss=2.1421 lr=8.00e-05
[train] epoch=0 step=4250 loss=2.2000 lr=7.50e-05
[train] epoch=0 step=4300 loss=2.1661 lr=7.00e-05
[train] epoch=0 step=4350 loss=2.1067 lr=6.50e-05
[train] epoch=0 step=4400 loss=2.1504 lr=6.00e-05
[train] epoch=0 step=4450 loss=2.1689 lr=5.50e-05
[train] epoch=0 step=4500 loss=2.1423 lr=5.00e-05
[train] epoch=0 step=4550 loss=2.1931 lr=4.50e-05
[train] epoch=0 step=4600 loss=2.1211 lr=4.00e-05
[train] epoch=0 step=4650 loss=2.1597 lr=3.50e-05
[train] epoch=0 step=4700 loss=2.0922 lr=3.00e-05
[train] epoch=0 step=4750 loss=2.0753 lr=2.50e-05
[train] epoch=0 step=4800 loss=2.1703 lr=2.00e-05
[train] epoch=0 step=4850 loss=2.1058 lr=1.50e-05
[train] epoch=0 step=4900 loss=2.1007 lr=1.00e-05
[train] epoch=0 step=4950 loss=2.0953 lr=5.00e-06
[train] epoch=0 step=5000 loss=2.2169 lr=0.00e+00
[val] epoch=0 val_loss=2.1364
[INFO] Saved checkpoint to checkpoints/encoder_pretrain/best_encoder.pt (step=5000, val_loss=2.1364)
[INFO] Saved loss plot to checkpoints/encoder_pretrain/training_losses.png

[INFO] Computing final evaluation metrics...

======================================================================
ENCODER CAPACITY EVALUATION METRICS
======================================================================

ðŸ“Š Reconstruction Performance:
  â€¢ Loss:              2.1382
  â€¢ Perplexity:        8.48
  â€¢ Token Accuracy:    30.27%
  â€¢ Sequence Accuracy: 0.00%

ðŸ”¬ Latent Space Statistics:
  â€¢ Mean (mu):         0.0185
  â€¢ Std (mu):          1.0891
  â€¢ Mean Variance:     0.7394
  â€¢ Pairwise Distance: 35.0258

ðŸ“ˆ Samples Evaluated: 1024
======================================================================

[INFO] Saved evaluation metrics to checkpoints/encoder_pretrain/evaluation_metrics.json

[INFO] Collecting latent space samples for visualization...
[INFO] Saved latent distribution plot to checkpoints/encoder_pretrain/latent_distribution.png

[INFO] Pretraining complete! Checkpoints and plots saved to checkpoints/encoder_pretrain
